our application is Berman meeting Solutions we created a meeting to elect combined speaker identification with speech recognition to create a transcript of any conversation or meeting me then perform natural language processing on this conversation to record meeting metrics and use conversational to use to complete Proactiv test for example let's send out a memo saying that this is a great meeting to all right what are my goals for this project is to eliminate the need for astromaster or the need for note taking so Denver are next picture I have an idea we should take a walk actually I don't like that idea let's take an Uber instead we utilize the Uber API to hell ride tomorrow position
 at the end of the meeting we can see a summary of different fares for local destinations process audio we directly record that uses audio in the browser natively at 44.1 K hurts send this audio to the backend which down samples at 216 K hurts we do this down down sampling through a command line tool called Socks we don't send the down sample audio to the Microsoft Azure speaker recognition API why is trained on our voices since during the rolling step we uploaded a 30-second audio of each of our voices the speech to text is done through the Google speech API we directly stream audio from the users browser to the speech API and how to correct mistakes we also buffer it
 at the  